# Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements

This is the implementation repository of our *ICSE'25* paper: **Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements**.



## 1. Description

Deep learning frameworks are the basis for developing and deploying deep learning applications. To improve the quality of deep learning frameworks, many researchers propose framework testing methods that generate deep learning models as test inputs. However, the effectiveness and efficiency of existing methods are restricted by the limitations in test input models, including low model diversity, too large model size, and low operator variety. To overcome these limitations, we propose a deep learning framework testing method via heuristic guidance based on multiple model measurements, namely, DLMOSA. Firstly, since diverse seed models contribute to promoting the diversity of test input models, DLMOSA proposes a mutation-based method to generate diverse seed models. In addition, to reduce model size and promote operator variety, DLMOSA proposes quantitative measurements for model size and operator variety respectively, and then designs a heuristic guidance for test input model generation based on the proposed two measurements. We apply DLMOSA to test three widely used deep learning frameworks (including TensorFlow, PyTorch, and MindSpore). The experimental results show that DLMOSA outperforms state-of-the-art methods in effectiveness and efficiency. The test input models generated by DLMOSA achieve 64.65\% improvement in model diversity and 250.13\% improvement in operator variety. In addition, the time consumption of DLMOSA is within 35\% of all state-of-the-art methods. DLMOSA newly detects 19 crashes and 175 NaN \& inconsistency bugs. Among detected crashes, 16 are confirmed by developers from the public community, and one is fixed in the following version.



You can access this repository using the following command:

```shell
git clone https://github.com/DeepLearningMOSA/DLMOSA.git
```



## 2. Framework version

We use three widely-used DL frameworks (*i.e.*, ***TensorFlow***, ***PyTorch***, and ***MindSpore***). We conduct the experimental environment as follow:

| TensorFlow | PyTorch | MindSpore | Keras |
| :--------: | :-----: | :-------: | :---: |
|   2.9.0    | 1.12.0  |   2.1.0   | 2.6.0 |



## 3. Environment

**Step 0:** Please install ***anaconda***.

**Step 1:** Create a conda environment. Run the following commands.

```sh
conda create -n DLMOSA python=3.9
source activate DLMOSA
pip install tensorflow==2.9.0
pip install mindspore==2.1.0
pip install torch==1.12.0
pip install keras==2.6.0
pip install networkx=3.2.1
```

We also list all requirements in the file `requirements.txt`.

## 4. File structure

This project contains six folders. The **LEMON-master** folder is the downloaded open source code for LEMON. The **Muffin-main** folder is the downloaded open source code for Muffin. The **COMET-master** folder is the downloaded open source code for COMET. The **Gandalf-main** folder is the downloaded open source code for Gandalf. The **DLMOSA** folder is the source code for our method. The **result** folder is the experimental result data. To know the execution methods of our baselines, please refer to the corresponding research papers. In this document, we will introduce how to run the source code for DLMOSA.

In the source code for DLMOSA, the folders named **DataStruct, Test, and Method** contain the body for the method. The program entry of the method is **main.py**. Run **main.py** to run DLMOSA after installing the experimental environment.

The folder named **result_analysis** is used in the Evaluation section, which will be explained in the following sections.

The folder named  **dataset** is used for preprocessing the dataset. DLMOSA supports six dataset, including MNIST, Fashion-MNIST, CIFAR-10, ImageNet, Sine-Wave and Stock-Price. In addition, DLMOSA also supports randomly generating test input tensors. The first three ones can be accessed by [Keras API](https://keras.io/api/datasets/)，while the rest can be access from [OneDrive](https://onedrive.live.com/?authkey=%21ANVR8C2wSN1Rb9M&id=34CB15091B189D3E%211909&cid=34CB15091B189D3E)(`dataset.zip`) provided by LEMON. Please set the variable *dataset* in the **globalConfig.py** in the folder **Datastruct** to specify the dataset.

## 5. Experiments

### 5.1 Main Method

Make sure you are now in the ***conda*** visual environment!

Use the following command to run the experiment according to the configuration:

```shell
python main.py
```

The testing results will be stored in `./new_result.csv`.

If you do not want to reproduce the experiment, experimental results are available in the folder **result**. There are three folders in the folder **result**: 1) Folder **abalation study** for the result of $DLMOSA_r$ and $DLMOSA_s$. 2) Folder **baselines（empirical study）** for all models generated by LEMON, Muffin, COMET, and Gandalf. 3) Folder **DLMOSA** for the exception messages for all crashes detected by our method, and the experimental results under each dataset (which is the fundamental data for detecting NaN & inconsistency bugs). 

### 5.2 Evaluation

#### 5.2.1 Evaluation for DLMOSA

**Step 1:** Set **file_path** and **result_csv** in **result_analysis/DLMOSA_Structure_and_precision_bugs.py** and **result_analysis/DLMOSA_edit_distance.py**. Especially, set *file_path* to the location of **new_result.csv** (newly generated). Set **result_csv** to the location of the structural analysis output.

**Step 2:** Use the following command:

```shell
python DLMOSA_Structure_and_precision_bugs.py
python DLMOSA_edit_distance.py
```

The structural analysis results of **DLMOSA_Structure_and_precision_bugs.py** will be stored in`./DLMOSA_graph_and_precision_bugs.csv`. The structural analysis results of **DLMOSA_edit_distance.py** will be exhibited in the terminal output.

5.2.2 Evaluation for LEMON, Muffin, COMET, and Gandalf

Generate model files by executing the open source code of their respective methods. Then convert these models to strings similar to those in **new_result.csv**. After that, run the corresponding analysis method. The configuration is similar to 5.2.1.

If you do not want to reproduce LEMON, Muffin, COMET, and Gandalf. All models generated are available in  the folder **result/baselines（empirical study）**. Set *file_path* and *result_csv* in the **result_analysis/COMET_edit_distance.py**, **result_analysis/COMET_Structure.py**, **result_analysis/COMET_Structure_and_precision_bugs.py** and run these files to evaluate COMET. Set *file_path* in the **result_analysis/Gandalf_variety.py**, **result_analysis/Lemon_variety.py**, **result_analysis/Muffin_edit_distance.py**, and **result_analysis/Muffin_variety.py** and run these files to evaluate Gandalf, LEMON, and Muffin. Specially, all *file_path* is set to the location of the model files in the folder **result/baselines（empirical study）**.
